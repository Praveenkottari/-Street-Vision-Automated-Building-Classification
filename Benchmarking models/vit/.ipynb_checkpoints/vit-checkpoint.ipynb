{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e9f2dd-2284-4893-be6e-589931760879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6309b0-1183-40fb-a12d-42a26f0f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dce1d05-1747-47eb-bc50-71d211bcf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = \"google/vit-base-patch16-224\"\n",
    "# vit_model = ViTModel.from_pretrained(\"google/vit-h-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9332b49-0330-4ce9-b8b9-e24c2f3a2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d77f51-f691-461d-9d55-9cb7bcfee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for grayscale images\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Grayscale(num_output_channels=1),  # Ensure the images are in grayscale (1 channel)\n",
    "    #transforms.RandomCrop(224),\n",
    "    transforms.Resize((224, 224)),  # Resize images to 300x400\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Use RGB normalization for pre-trained models\n",
    "                         std=[0.229, 0.224, 0.225])  # Normalize grayscale images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedab4e1-b977-4871-a409-e14b8edf594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './../dataset/raw_data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1d9f09-efce-4366-9334-d588dcde5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir)\n",
    "\n",
    "# Ensure the dataset classes are in the correct order (1, 2, 3, 4, 5)\n",
    "dataset.classes.sort(key=lambda x: int(x))  # Sort the class names numerically if they are numeric strings\n",
    "\n",
    "# Create a mapping from numerical labels to class names\n",
    "label_to_class_name = {idx: class_name for idx, class_name in enumerate(dataset.classes)}\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Assign transformations\n",
    "train_dataset.dataset.transform = transform\n",
    "val_dataset.dataset.transform = transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b06ec64-dfd1-49bc-87b5-0cbb25f27429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa0dc1a-3df6-4865-b28c-355939934cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(vit_model, num_labels=5, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3d6600-26b8-4f36-ac75-14b7b4d99d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#class_counts = [245,270,493,595,245]\n",
    "# class_counts = [624, 716, 714, 1112,508] #augmented model\n",
    "class_counts = [996,1138,1136,1766,807] #aug-raw mix\n",
    "# class_counts = [312,358,357,556,254] #canny\n",
    "\n",
    "total_samples = sum(class_counts)\n",
    "class_weights = [total_samples / (len(class_counts) * count) for count in class_counts]\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba1dad1-3c88-4e3b-9981-1a80993ac4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af19cbf2-6e8c-46e6-8bc6-35b0f0b920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0966a-91f2-471e-9dac-b2a5f2987418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44857c77-214c-413c-980b-05f4231c7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b2d84b8-a491-4446-a38c-35cffaae273a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:831\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 831\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    842\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:614\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m    610\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    611\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    612\u001b[0m )\n\u001b[0;32m--> 614\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:443\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    436\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    437\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    438\u001b[0m         hidden_states,\n\u001b[1;32m    439\u001b[0m         layer_head_mask,\n\u001b[1;32m    440\u001b[0m         output_attentions,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:400\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    397\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attention_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(layer_output)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/modules/normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_pk/lib/python3.10/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        #loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        # Track training loss and accuracy\n",
    "        running_train_loss += loss.item()\n",
    "        #_, predicted = torch.max(outputs, 1)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_train_loss / len(train_loader)\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # Track validation loss and accuracy\n",
    "            running_val_loss += loss.item()\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cf6aa-143d-4702-81c2-b3a25b23210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "371c0ca3-8e37-433e-ad65-b4459a248833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a6368892-dd94-4e20-9c2d-52fe47a4c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56        45\n",
      "           1       0.35      0.44      0.39        48\n",
      "           2       0.56      0.48      0.52        86\n",
      "           3       0.80      0.88      0.84       105\n",
      "           4       0.66      0.52      0.58        44\n",
      "\n",
      "    accuracy                           0.62       328\n",
      "   macro avg       0.58      0.57      0.58       328\n",
      "weighted avg       0.62      0.62      0.61       328\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits \n",
    "            _, preds = torch.max(logits, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Get predictions and true labels\n",
    "true_labels, predictions = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[1, 2, 3, 4,5],  # Adjust class labels as necessary\n",
    "            yticklabels=[1, 2, 3, 4,5])  # Adjust class labels as necessary\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ce37b70d-86a4-4183-b04c-d0d89f0d1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = './weights/vit-raw3.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b96bbc69-9334-4ad7-b3ac-7783fff3e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 10  9  1  0]\n",
      " [11 21 11  4  1]\n",
      " [ 6 17 41 14  8]\n",
      " [ 2  1  7 92  3]\n",
      " [ 1 11  5  4 23]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c250c-2492-48bf-9f99-28bf06eb0e7d",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "55d1fdb5-b556-4690-9a71-8f14e39b1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "95da23d9-5253-4538-a1c8-c3dfdf9adbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1602544/3243614887.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model (make sure it's in evaluation mode)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights_path = './weights/vit-raw3.pth'\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=5, ignore_mismatched_sizes=True)\n",
    "# model.load_state_dict(torch.load('./weights/vit-aug3.pth'))\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4623b49a-6f46-4665-9118-9f6f02b0c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for test images\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 300x400\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB if needed\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize RGB images\n",
    "])\n",
    "\n",
    "# Directory containing test images\n",
    "test_dir = './dataset/test'\n",
    "test_images = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "72520ec9-649e-46d4-960b-b2f3a62a2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort filenames to ensure correct order\n",
    "test_images.sort(key=lambda x: int(os.path.splitext(x)[0]))  # Assuming filenames are numeric\n",
    "\n",
    "# Initialize lists to store filenames and predictions\n",
    "filenames = []\n",
    "predictions = []\n",
    "\n",
    "# Process each image in the test directory\n",
    "for image_name in test_images:\n",
    "    image_path = os.path.join(test_dir, image_name)\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "    image = test_transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Predict the class\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        class_id = predicted.item()\n",
    "\n",
    "    # Save results\n",
    "    filenames.append(os.path.splitext(image_name)[0])  # Remove .jpg from filename\n",
    "    predictions.append(class_id + 1)  # Map class_id to 1-based index if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "157e894b-5745-4e50-8178-f0e70348297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame({'ID': filenames, 'Predictions': predictions})\n",
    "results_df.to_csv('predictions-swin-raw3.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8dc51-b354-49da-adbb-c71cb4764d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66aede-1be8-4b1f-a8f9-d1c680464055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
